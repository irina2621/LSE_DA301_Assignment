{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59711a08",
   "metadata": {},
   "source": [
    "### LSE Data Analytics Online Career Accelerator \n",
    "\n",
    "# TURTLE GAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90776e",
   "metadata": {},
   "source": [
    "### Questions to answer:\n",
    "Turtle Games, a game manufacturer and retailer. They manufacture and sell their own products, along with sourcing and selling products manufactured by other companies. Their product range includes books, board games, video games and toys. They have a global customer base and have a business objective of improving overall sales performance by utilising customer trends. In particular, Turtle Games wants to understand: \n",
    "- how customers accumulate loyalty points.\n",
    "- how useful are remuneration and spending scores data\n",
    "- can social data (e.g. customer reviews) be used in marketing campaigns\n",
    "- what is the impact on sales per product\n",
    "- the reliability of the data (e.g. normal distribution, Skewness, Kurtosis)\n",
    "- if there is any possible relationship(s) in sales between North America, Europe, and global sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75093b",
   "metadata": {},
   "source": [
    "## How customers accumulate loyalty points?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdfeaee",
   "metadata": {},
   "source": [
    "The marketing department of Turtle Games prefers Python for data analysis. Data analysis of social media data were made. The marketing department wants to better understand how users accumulate loyalty points. Therefore, we investigate the possible relationships between the loyalty points, age, remuneration, and spending scores. \n",
    "\n",
    "## Instructions\n",
    "1. Load and explore the data.\n",
    "    1. Create a new DataFrame (e.g. reviews).\n",
    "    2. Sense-check the DataFrame.\n",
    "    3. Determine if there are any missing values in the DataFrame.\n",
    "    4. Create a summary of the descriptive statistics.\n",
    "2. Remove redundant columns (`language` and `platform`).\n",
    "3. Change column headings to names that are easier to reference (e.g. `renumeration` and `spending_score`).\n",
    "4. Save a copy of the clean DataFrame as a CSV file. Import the file to sense-check.\n",
    "5. Use linear regression and the `statsmodels` functions to evaluate possible linear relationships between loyalty points and age/renumeration/spending scores to determine whether these can be used to predict the loyalty points.\n",
    "    1. Specify the independent and dependent variables.\n",
    "    2. Create the OLS model.\n",
    "    3. Extract the estimated parameters, standard errors, and predicted values.\n",
    "    4. Generate the regression table based on the X coefficient and constant values.\n",
    "    5. Plot the linear regression and add a regression line.\n",
    "6. Include your insights and observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea2c71",
   "metadata": {},
   "source": [
    "## 1. Load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cc3186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports libraries needed for analysis.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm \n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c3d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the turtle_reviews.csv as reviews dataframe.\n",
    "reviews = pd.read_csv('turtle_reviews.csv')\n",
    "\n",
    "# View the dataframe.\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609db504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the dataframe.\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f96de1-888a-4698-9ec1-23e768642848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any missing data presented in the dataframe.\n",
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f36bd0-71be-4494-b873-1b1a2f52f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data.\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad16658-b4eb-4080-8084-ea043453ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore descriptive statistics for numeric data.\n",
    "reviews.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d25344d-3aed-4d27-bb24-2142be9c99ef",
   "metadata": {},
   "source": [
    "## 2. Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b813a-f04f-4c3a-9a11-ad6d7a423525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove redundant columns (language and platform).\n",
    "reviews = reviews.drop(['language', 'platform'], axis=1)\n",
    "\n",
    "# View column names and chck the shape of the dataframe.\n",
    "print(reviews.shape)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fafd556-c6fa-439b-aac3-0fe332b1eb45",
   "metadata": {},
   "source": [
    "## 3. Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06310ed-ab6b-4f6e-8307-bdd3380853f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns (\"remuneration (k£)\" and \"spending_score (1-100)\") headers.\n",
    "reviews.rename(columns={\"remuneration (k£)\":\"remuneration\", \"spending_score (1-100)\":\"spending_score\"}, inplace=True)\n",
    "\n",
    "# View column names.\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c386d53-d38c-4b24-8883-7d2257320036",
   "metadata": {},
   "source": [
    "## 4. Save the DataFrame as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc1746-570a-47cc-a8a9-fe8b6756a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSV file as output.\n",
    "reviews.to_csv(\"reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d4f35-c1b3-40ab-ba63-c5fc551f3ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import new CSV file with Pandas.\n",
    "reviews = pd.read_csv('reviews.csv')\n",
    "\n",
    "# View the DataFrame.\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e17c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sense check the dataframe.\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdd7d5f-2501-4e3e-895c-4a02602e078a",
   "metadata": {},
   "source": [
    "## 5. Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ccb06",
   "metadata": {},
   "source": [
    "Let's evaluate possible linear relationships between loyalty points and spending scores/remuneration/age to determine whether these can be used to predict the loyalty points?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a7f47e",
   "metadata": {},
   "source": [
    "### 5a) spending vs loyalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75863d52-79df-4200-b044-5542db990fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependent variable (y) is loyalty points.\n",
    "y = reviews['loyalty_points'] \n",
    "\n",
    "# Independent variable (x) is spending score.\n",
    "x = reviews['spending_score']\n",
    "\n",
    "# Create formula and pass through OLS methods.\n",
    "f = 'y ~ x'\n",
    "test = ols(f, data = reviews).fit()\n",
    "\n",
    "# Print the regression table.\n",
    "test.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dfb637",
   "metadata": {},
   "source": [
    "R 2 : 45.2% of the total variability of y (loyalty points the customers have), is explained by the variability of X (how much they spent or spending score). X: The coefficient of X describes the slope of the regression line or how much the response variable y change when X changes by 1 unit. So, if the spending score that the customer has (X) changes by 1 unit the loyalty (y) will change by 33.0617 units. The t-value tests the hypothesis that the slope is significant or not. If the corresponding probability is small (typically smaller than 0.05) the slope is significant. In this case, the probability of the t-value is zero, thus the estimated slope is significant. The last two numbers describe the 95% confidence interval of the true x coefficient, i.e. the true slope. For instance, if you take a different sample, the estimated slope will be slightly different. If you take 100 random samples each of 500 observations of X and y, then 95 out of the 100 samples will derive a slope that is within the interval (31.464, 34.659)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a55354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the estimated parameters.\n",
    "print(\"Parameters: \", test.params) \n",
    "\n",
    "# Extract the standard errors.\n",
    "print(\"Standard errors: \", test.bse)  \n",
    "\n",
    "# Extract the predicted values.\n",
    "print(\"Predicted values: \", test.predict()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9798e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the X coefficient and the constant to generate the regression table.\n",
    "y_pred = -75.052663 + 33.061693 * reviews['spending_score']\n",
    "\n",
    "# View the output.\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dde67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data points with a scatterplot.\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# Plot the regression line (in black).\n",
    "plt.plot(x, y_pred, color='red')\n",
    "\n",
    "# Set the x and y limits on the axes.\n",
    "plt.xlim(0)\n",
    "plt.ylim(0)\n",
    "\n",
    "# View the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc0f24f",
   "metadata": {},
   "source": [
    "### 5b) remuneration vs loyalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db590005-b90a-4005-875e-dbec56155229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependent variable (y) is loyalty points.\n",
    "y = reviews['loyalty_points'] \n",
    "\n",
    "# Independent variable (x) is remuniration.\n",
    "x = reviews['remuneration']\n",
    "\n",
    "# Create formula and pass through OLS methods.\n",
    "f = 'y ~ x'\n",
    "test = ols(f, data = reviews).fit()\n",
    "\n",
    "# Print the regression table.\n",
    "test.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af00eea6",
   "metadata": {},
   "source": [
    "R 2 : 38.0% of the total variability of y (loyalty points the customers have), is explained by the variability of X (remuniration). X: The coefficient of X describes the slope of the regression line or how much the response variable y change when X changes by 1 unit. So, if the remuniration of the customer has (X) changes by 1 unit the loyalty point (y) will change by 34.1878 units. The t-value tests the hypothesis that the slope is significant or not. If the corresponding probability is small (typically smaller than 0.05) the slope is significant. In this case, the probability of the t-value is zero, thus the estimated slope is significant. The last two numbers describe the 95% confidence interval of the true xcoefficient, i.e. the true slope. For instance, if you take a different sample, the estimated slope will be slightly different. If you take 100 random samples each of 500 observations of X and y, then 95 out of the 100 samples will derive a slope that is within the interval (32.270, 36.106)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e9ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the estimated parameters.\n",
    "print(\"Parameters: \", test.params) \n",
    "\n",
    "# Extract the standard errors.\n",
    "print(\"Standard errors: \", test.bse)  \n",
    "\n",
    "# Extract the predicted values.\n",
    "print(\"Predicted values: \", test.predict()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291fb0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the X coefficient and the constant to generate the regression table.\n",
    "y_pred = -65.686513 + 34.187825 * reviews['remuneration']\n",
    "\n",
    "# View the output.\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d00e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data points with a scatterplot.\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# Plot the regression line (in black).\n",
    "plt.plot(x, y_pred, color='red')\n",
    "\n",
    "# Set the x and y limits on the axes.\n",
    "plt.xlim(0)\n",
    "plt.ylim(0)\n",
    "\n",
    "# View the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e1552d",
   "metadata": {},
   "source": [
    "### 5c) age vs loyalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099274ee-8c86-44dc-a8dc-dfbf91e59728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependable variable (x) is loyalty points.\n",
    "y = reviews['loyalty_points'] \n",
    "\n",
    "# Independent variable (x) is age.\n",
    "x = reviews['age']\n",
    "\n",
    "# Create formula and pass through OLS methods.\n",
    "f = 'y ~ x'\n",
    "test = ols(f, data = reviews).fit()\n",
    "\n",
    "# Print the regression table.\n",
    "test.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a86a9",
   "metadata": {},
   "source": [
    "R 2 : Only 2.0% of the total variability of y (loyalty points the customers have), is explained by the variability of X (customer age). X: The coefficient of X describes the slope of the regression line or how much the response variable y change when X changes by 1 unit. So, if the age of the customer has (X) changes by 1 unit the loyalty point (y) will change by -4.0128 units. The t-value tests the hypothesis that the slope is significant or not. In this case, t-value is 0.058, thus the estimated slope is not significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0084f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the estimated parameters.\n",
    "print(\"Parameters: \", test.params) \n",
    "\n",
    "# Extract the standard errors.\n",
    "print(\"Standard errors: \", test.bse)  \n",
    "\n",
    "# Extract the predicted values.\n",
    "print(\"Predicted values: \", test.predict()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the X coefficient and the constant to generate the regression table.\n",
    "y_pred = 1736.517739 - 4.012805 * reviews['age']\n",
    "\n",
    "# View the output.\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data points with a scatterplot.\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# Plot the regression line (in black).\n",
    "plt.plot(x, y_pred, color='red')\n",
    "\n",
    "# Set the x and y limits on the axes.\n",
    "plt.xlim(0)\n",
    "plt.ylim(0)\n",
    "\n",
    "# View the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e65913",
   "metadata": {},
   "source": [
    "### 5d) Multiple Linear Regression (MLR): remuneration and spending score vs loyalty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0143bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary packages.\n",
    "import statsmodels.stats.api as sms\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2bf548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dependent variable y loyalty points.\n",
    "y = reviews['loyalty_points']  \n",
    "\n",
    "# Define the independent variables X.\n",
    "X = reviews[['remuneration', 'spending_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b7feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the regression model.\n",
    "multi = LinearRegression()  \n",
    "multi.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d0fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the predictions for X (array).\n",
    "multi.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2329f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the value of R-squared, intercept and coefficients in the model.\n",
    "print(\"R-squared: \", multi.score(X, y))\n",
    "print(\"Intercept: \", multi.intercept_)\n",
    "print(\"Coefficients:\")\n",
    "\n",
    "list(zip(X, multi.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c73f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions. Create a variales new_remuneration and new_spending_score.\n",
    "new_remuneration = 22.96\n",
    "new_spending_score = 61\n",
    "new_age = 37\n",
    "print ('Predicted Value: \\n', multi.predict([[new_remuneration ,new_spending_score]]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test subsets with (MLR) multiple linear regression\n",
    "# Split the data in 'train' (80%) and 'test' (20%) sets.\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, y,\n",
    "                                                                            test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1809e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model using the 'statsmodel' OLS library.\n",
    "# Fit the model with the added constant.\n",
    "model = sm.OLS(Y_train, sm.add_constant(X_train)).fit()\n",
    "\n",
    "# Set the predicted response vector.\n",
    "Y_pred = model.predict(sm.add_constant(X_test)) \n",
    "\n",
    "# Call a summary of the model.\n",
    "print_model = model.summary()\n",
    "\n",
    "# Print the summary.\n",
    "print(print_model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fd84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model.\n",
    "mlr = LinearRegression()  \n",
    "\n",
    "# Fit the model. Use training data set.\n",
    "mlr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a36cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the predictions for X in the test set.\n",
    "y_pred_mlr = mlr.predict(X_train)  \n",
    "\n",
    "# Print the predictions.\n",
    "print(\"Prediction for train set: {}\".format(y_pred_mlr)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bda09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the predictions for X in the test set.\n",
    "y_pred_mlr = mlr.predict(X_test)  \n",
    "\n",
    "# Print the predictions.\n",
    "print(\"Prediction for test set: {}\".format(y_pred_mlr)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3519931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the R-squared value.\n",
    "print(mlr.score(X_test, Y_test)*100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8f9028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for multicollinearity.\n",
    "x_temp = sm.add_constant(X_train)\n",
    "\n",
    "# Create an empty DataFrame. \n",
    "vif = pd.DataFrame()\n",
    "\n",
    "# Calculate the VIF for each value.\n",
    "vif['VIF Factor'] = [variance_inflation_factor(x_temp.values,\n",
    "                                               i) for i in range(x_temp.values.shape[1])]\n",
    "\n",
    "# Create the feature columns.\n",
    "vif['features'] = x_temp.columns\n",
    "\n",
    "# Print the values to one decimal points.\n",
    "print(vif.round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1bee22",
   "metadata": {},
   "source": [
    "VIF factors for all 2 variables are close to 1, therefore multicollinearity is not a problem here.\n",
    "No correlation or very weak correlation between variables were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc86c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine heteroscedasticity.\n",
    "model = sms.het_breuschpagan(model.resid, model.model.exog) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395a738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = ['LM stat', 'LM Test p-value', 'F-stat', 'F-test p-value']\n",
    "print(dict(zip(terms, model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9325d62",
   "metadata": {},
   "source": [
    "The Lagrange multiplier statistic for the test is 38.016 and the corresponding p-value is less than 0.05, we reject the null hypothesis (homoscedasticity is present)and have sufficient evidence to say that heteroscedasticity is present in the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89443b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the metrics.mean_absolute_error function.  \n",
    "print('Mean Absolute Error (Final):', metrics.mean_absolute_error(Y_test, Y_pred))  \n",
    "\n",
    "# Call the metrics.mean_squared_error function.\n",
    "print('Mean Square Error (Final):', metrics.mean_squared_error(Y_test, Y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1fa5b5-ca8f-4ee8-a22e-532142d5cf52",
   "metadata": {},
   "source": [
    "## 6. Observations and insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713d8150-b5e5-4b96-b138-3437158bc020",
   "metadata": {},
   "source": [
    "After sense checking and cleaning the given dataset, we tried to find if there are any relationships between the loyalty point and spending score/remuniration/age. Ordinary Least Squares (OLS) to estimate a linear regression model and fit a linear equation to observed data was used. The results are below:\n",
    "\n",
    "*spending vs loyalty: y_pred = -75.052663 + 33.061693 * reviews['spending_score']*\n",
    "\n",
    "*remuneration vs loyalty: y_pred = -65.686513 + 34.187825 * reviews['remuneration']*\n",
    "\n",
    "*age vs loyalty: y_pred = 1736.517739 - 4.012805 * reviews['age']*\n",
    "\n",
    "Only in the first two coefficients were significant according to t-value test. The last one(age vs loyalty) where the coefficient is actualy negative as well, the probability of  the t-value is 0.058>0.05, thus its not statistically significant. \n",
    "\n",
    "Spending vs loyalty has R2 is 45.2% and in remuneration vs loyalty it is 38%. Both are not great to predict loyalty. In order to improve the model we can try Multivariate regression model (MRL), taking all variables into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7608bfd2",
   "metadata": {},
   "source": [
    " As a result we've got much higher R2, which means 82,8% of loyalty points variability is explained by variability of spending score, remuneration and age together. All coefficients are statistically significant, according to probability of t-value (0.00<0.05). Remuneration is 33.7573 (so if it changes 1 unit, the value of y (loyalty points) increases by 33.7573 units). Spending score cofficient is similar 32.948.\n",
    "As VIF is close to 1.0 for all independant variables meaning no correlation found. We also check for heteroscedasticity, as it is a problem because ordinary least squares (OLS) regression assumes that the residuals come from a population that has homoscedasticity, which means constant variance. In our example it is present. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e28c75",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5999c07d",
   "metadata": {},
   "source": [
    "## How useful are remuneration and spending scores data? Defining customer groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c924ec",
   "metadata": {},
   "source": [
    "# Clustering with *k*-means using Python\n",
    "\n",
    "The marketing department also wants to better understand the usefulness of renumeration and spending scores but do not know where to begin. We tasked to identify groups within the customer base that can be used to target specific market segments. Use *k*-means clustering to identify the optimal number of clusters and then apply and plot the data using the created segments.\n",
    "\n",
    "## Instructions\n",
    "1. Prepare the data for clustering. \n",
    "    1. Import the CSV file you have prepared in Week 1.\n",
    "    2. Create a new DataFrame (e.g. `df2`) containing the `renumeration` and `spending_score` columns.\n",
    "    3. Explore the new DataFrame. \n",
    "2. Plot the renumeration versus spending score.\n",
    "    1. Create a scatterplot.\n",
    "    2. Create a pairplot.\n",
    "3. Use the Silhouette and Elbow methods to determine the optimal number of clusters for *k*-means clustering.\n",
    "    1. Plot both methods and explain how you determine the number of clusters to use.\n",
    "    2. Add titles and legends to the plot.\n",
    "4. Evaluate the usefulness of at least three values for *k* based on insights from the Elbow and Silhoutte methods.\n",
    "    1. Plot the predicted *k*-means.\n",
    "    2. Explain which value might give you the best clustering.\n",
    "5. Fit a final model using your selected value for *k*.\n",
    "    1. Justify your selection and comment on the respective cluster sizes of your final solution.\n",
    "    2. Check the number of observations per predicted class.\n",
    "6. Plot the clusters and interpret the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e7299b",
   "metadata": {},
   "source": [
    "## 1. Load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1dc705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2645d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file(s) as df2.\n",
    "df2 = pd.read_csv('reviews.csv')\n",
    "\n",
    "# View the DataFrame.\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4317dbb3-bab1-4ad3-9c43-ee9af8ffcc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns.\n",
    "df2 = df2.drop(['review', 'summary'], axis = 1)\n",
    "\n",
    "# View DataFrame.\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f98e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data.\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e10d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics.\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f971229",
   "metadata": {},
   "source": [
    "## 2. Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot with Seaborn.\n",
    "# Import Seaborn and Matplotlib.\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a scatterplot with Seaborn.\n",
    "sns.scatterplot(x='remuneration',\n",
    "                y='spending_score',\n",
    "                data=df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600783ce-5a91-4ca8-993f-29499fcc4918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot with Seaborn.\n",
    "x = df2[['remuneration', 'spending_score']]\n",
    "\n",
    "sns.pairplot(df2,\n",
    "             vars=x,\n",
    "             diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d64b28-fc72-4633-af71-2040de573ece",
   "metadata": {},
   "source": [
    "## 3. Elbow and silhoutte methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd0290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of clusters: Elbow method.\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Elbow chart for us to decide on the number of optimal clusters.\n",
    "ss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i,\n",
    "                    init='k-means++',\n",
    "                    max_iter=500,\n",
    "                    n_init=10,\n",
    "                    random_state=0)\n",
    "    kmeans.fit(x)\n",
    "    ss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow method.\n",
    "plt.plot(range(1, 11),\n",
    "         ss,\n",
    "         marker='o')\n",
    "\n",
    "# Insert labels and title.\n",
    "plt.title(\"The Elbow Method\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"SS distance\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d91db92",
   "metadata": {},
   "source": [
    "Elbow method used to determine k number, when sum of square distances from each point to its assigned centre of the cluster is plotted on Y-axis, and X-axis is a number of clusters. 4-6 cluseters seem to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce995702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of clusters: Silhouette method.\n",
    "# Import silhouette_score class from sklearn.\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Find the range of clusters to be used using silhouette method.\n",
    "sil = []\n",
    "kmax = 10\n",
    "\n",
    "for k in range(2, kmax+1):\n",
    "    kmeans_s = KMeans(n_clusters=k).fit(x)\n",
    "    labels = kmeans_s.labels_\n",
    "    sil.append(silhouette_score(x,\n",
    "                                labels,\n",
    "                                metric='euclidean'))\n",
    "\n",
    "# Plot the silhouette method.\n",
    "plt.plot(range(2, kmax+1),\n",
    "         sil,\n",
    "         marker='o')\n",
    "\n",
    "# Insert labels and title.\n",
    "plt.title(\"The Silhouette Method\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Sil\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df53003",
   "metadata": {},
   "source": [
    "Silhoutte method gives 5-6 clusters as optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0fd764",
   "metadata": {},
   "source": [
    "## 4. Evaluate k-means model at different values of *k*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe46db99",
   "metadata": {},
   "source": [
    "## 4a) Use 4 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e340aa-fac0-4cd1-8da0-ee5502a81504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use four clusters.\n",
    "kmeans = KMeans(n_clusters = 4, \n",
    "                max_iter = 15000,\n",
    "                init='k-means++',\n",
    "                random_state=42).fit(x)\n",
    "\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "x['K-Means Predicted'] = clusters\n",
    "\n",
    "# Plot the predicted.\n",
    "sns.pairplot(x,\n",
    "             hue='K-Means Predicted',\n",
    "             diag_kind= 'kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0869d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of observations per predicted class.\n",
    "x['K-Means Predicted'].value_counts().sort_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce6260b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View the K-Means predicted.\n",
    "print(x.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0af292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the clusters.\n",
    "# Set plot size.\n",
    "sns.set(rc = {'figure.figsize':(12, 8)})\n",
    "\n",
    "# Create a scatterplot.\n",
    "sns.scatterplot(x='remuneration',\n",
    "                y='spending_score',\n",
    "                data=x,\n",
    "                hue='K-Means Predicted',\n",
    "                palette=['red', 'green', 'blue','black'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dd4ffc",
   "metadata": {},
   "source": [
    "## 4b) Use 5 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66197d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use five clusters.\n",
    "kmeans = KMeans(n_clusters = 5, \n",
    "                max_iter = 15000,\n",
    "                init='k-means++',\n",
    "                random_state=42).fit(x)\n",
    "\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "x['K-Means Predicted'] = clusters\n",
    "\n",
    "# Plot the predicted.\n",
    "sns.pairplot(x,\n",
    "             hue='K-Means Predicted',\n",
    "             diag_kind= 'kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbce0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of observations per predicted class.\n",
    "x['K-Means Predicted'].value_counts().sort_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15fce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the K-Means predicted.\n",
    "print(x.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a641ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the clusters.\n",
    "# Set plot size.\n",
    "sns.set(rc = {'figure.figsize':(12, 8)})\n",
    "\n",
    "# Create a scatterplot.\n",
    "sns.scatterplot(x='remuneration',\n",
    "                y='spending_score',\n",
    "                data=x,\n",
    "                hue='K-Means Predicted',\n",
    "                palette=['red', 'green', 'blue','black','purple'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff9a5a3",
   "metadata": {},
   "source": [
    "## 4c) Use 6 clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bdf570-062b-488c-9043-1372d3f6c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use six clusters.\n",
    "kmeans = KMeans(n_clusters = 6, \n",
    "                max_iter = 15000,\n",
    "                init='k-means++',\n",
    "                random_state=42).fit(x)\n",
    "\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "x['K-Means Predicted'] = clusters\n",
    "\n",
    "# Plot the predicted.\n",
    "sns.pairplot(x,\n",
    "             hue='K-Means Predicted',\n",
    "             diag_kind= 'kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c592809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of observations per predicted class.\n",
    "x['K-Means Predicted'].value_counts().sort_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeea03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the K-Means predicted.\n",
    "print(x.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31caff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the clusters.\n",
    "# Set plot size.\n",
    "sns.set(rc = {'figure.figsize':(12, 8)})\n",
    "\n",
    "# Create a scatterplot.\n",
    "sns.scatterplot(x='remuneration',\n",
    "                y='spending_score',\n",
    "                data=x,\n",
    "                hue='K-Means Predicted',\n",
    "                palette=['red', 'green', 'blue','black','purple','pink'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5625ce",
   "metadata": {},
   "source": [
    "## 5. Discuss: Insights and observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c154395",
   "metadata": {},
   "source": [
    "It seems that k=5 (five clusters) might give the best results (groups) to target specific market segments based on remuneration and spending score. Cluster 0 is the largest group, as many users have remuneration and spending score about the mean value. The number of predicted values per class indicates a better distribution for k=5 than k=6 or k=4 (we compare predicted values with the data pairplot). Later scatterplot based on prediction of cluster membership is used to visually see the separation of predictive classification types based on remuneration and spending score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7931bd0f",
   "metadata": {},
   "source": [
    "## Use of social data (customer reviews and summaries) for marketing campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b58c90",
   "metadata": {},
   "source": [
    "# NLP using Python\n",
    "Customer reviews were downloaded from the website of Turtle Games. This data will be used to steer the marketing department on how to approach future campaigns. Therefore, the marketing department asked you to identify the 15 most common words used in online product reviews. They also want to have a list of the top 20 positive and negative reviews received from the website. Therefore, you need to apply NLP on the data set.\n",
    "\n",
    "## Instructions\n",
    "1. Load and explore the data. \n",
    "    1. Sense-check the DataFrame.\n",
    "    2. You only need to retain the `review` and `summary` columns.\n",
    "    3. Determine if there are any missing values.\n",
    "2. Prepare the data for NLP\n",
    "    1. Change to lower case and join the elements in each of the columns respectively (`review` and `summary`).\n",
    "    2. Replace punctuation in each of the columns respectively (`review` and `summary`).\n",
    "    3. Drop duplicates in both columns (`review` and `summary`).\n",
    "3. Tokenise and create wordclouds for the respective columns (separately).\n",
    "    1. Create a copy of the DataFrame.\n",
    "    2. Apply tokenisation on both columns.\n",
    "    3. Create and plot a wordcloud image.\n",
    "4. Frequency distribution and polarity.\n",
    "    1. Create frequency distribution.\n",
    "    2. Remove alphanumeric characters and stopwords.\n",
    "    3. Create wordcloud without stopwords.\n",
    "    4. Identify 15 most common words and polarity.\n",
    "5. Review polarity and sentiment.\n",
    "    1. Plot histograms of polarity (use 15 bins) for both columns.\n",
    "    2. Review the sentiment scores for the respective columns.\n",
    "6. Identify and print the top 20 positive and negative reviews and summaries respectively.\n",
    "7. Include your insights and observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40558b5f",
   "metadata": {},
   "source": [
    "## 1. Load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary packages.\n",
    "import nltk \n",
    "import os \n",
    "\n",
    "nltk.download ('punkt')\n",
    "nltk.download ('stopwords')\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Import Counter.\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85947561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set as dataframe df3.\n",
    "df3 = pd.read_csv('reviews.csv')\n",
    "\n",
    "# View the DataFrame.\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c8b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data set. Define size and datatypes.\n",
    "df3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78f7176",
   "metadata": {},
   "source": [
    " Initial data set has 2000 observations and 9 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1694122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep necessary columns (only reviews and summary). Drop unnecessary columns.\n",
    "df3 = df3.drop(['gender','age','remuneration','spending_score','loyalty_points','education','product'], axis=1)\n",
    "\n",
    "# View DataFrame.\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00736320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if there are any missing values in the dataframe.\n",
    "df3.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936bd63b",
   "metadata": {},
   "source": [
    "## 2. Prepare the data for NLP\n",
    "### 2a) Change to lower case and join the elements in each of the columns respectively (review and summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review column: Change all to lower case and join with a space.\n",
    "df3['review'] = df3['review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "# View the result.\n",
    "df3['review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615be2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary column: Change all to lower case and join with a space.\n",
    "df3['summary'] = df3['summary'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "# View the result.\n",
    "df3['summary'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be5b39d",
   "metadata": {},
   "source": [
    "### 2b) Replace punctuation in each of the columns respectively (review and summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e14ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all the punctuations in review column with blank spaces.\n",
    "df3['review'] = df3['review'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# View output.\n",
    "df3['review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e003fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all the puncuations in summary column with blank spaces.\n",
    "df3['summary'] = df3['summary'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# View output.\n",
    "df3['summary'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb6d0b2",
   "metadata": {},
   "source": [
    "### 2c) Drop duplicates in both columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for duplicates in review column.\n",
    "df3.review.duplicated().sum()\n",
    "\n",
    "# Drop duplicates in review column.\n",
    "df3 = df3.drop_duplicates(subset=['review'])\n",
    "\n",
    "# View DataFrame.\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0521df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for duplicates in summary columns.\n",
    "df3.summary.duplicated().sum()\n",
    "\n",
    "# Drop duplicates in summary column.\n",
    "df3 = df3.drop_duplicates(subset=['summary'])\n",
    "\n",
    "# View DataFrame.\n",
    "df3.reset_index(inplace=True)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d730a3b5",
   "metadata": {},
   "source": [
    " After all cleaning 1349 records left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee064a9-dbc4-4b82-b6e7-17c6e441fa05",
   "metadata": {},
   "source": [
    "## 3. Tokenise and create wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5111dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame (copy DataFrame).\n",
    "df4 = df3\n",
    "\n",
    "# View DataFrame.\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ace8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenisation to both columns (review and summary) and assign them to new columns.\n",
    "df4['token_review'] = df4['review'].apply(word_tokenize)\n",
    "df4['token_summary'] = df4['summary'].apply(word_tokenize)\n",
    "\n",
    "# View DataFrame.\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review: Create a word cloud.\n",
    "# Create an empty string variable for all reviews.\n",
    "all_review = ''\n",
    "for i in range(df4.shape[0]):\n",
    "    # Add each comment.\n",
    "    all_review = all_review + df4['review'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review: Plot the WordCloud image.\n",
    "# Set the colour palette.\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# Create a WordCloud object (including stopwords).\n",
    "word_cloud = WordCloud(width = 1600, height = 900, \n",
    "                background_color ='black',\n",
    "                colormap = 'plasma', \n",
    "                stopwords = 'none',\n",
    "                min_font_size = 10).generate(all_review) \n",
    "\n",
    "# Plot the WordCloud image.                    \n",
    "plt.figure(figsize = (16, 9), facecolor = None) \n",
    "plt.imshow(word_cloud) \n",
    "plt.axis('off') \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cbdfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: Create a word cloud.\n",
    "# Create an empty string variable for all summaries.\n",
    "all_summary = ''\n",
    "for i in range(df4.shape[0]):\n",
    "    # Add each comment.\n",
    "    all_summary = all_summary + df4['summary'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53776cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: Plot the WordCloud image.\n",
    "# Set the colour palette.\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# Create a WordCloud object (including stopwords).\n",
    "word_cloud = WordCloud(width = 1600, height = 900, \n",
    "                background_color ='white',\n",
    "                colormap = 'plasma', \n",
    "                stopwords = 'none',\n",
    "                min_font_size = 10).generate(all_summary) \n",
    "\n",
    "# Plot the WordCloud image.                    \n",
    "plt.figure(figsize = (16, 9), facecolor = None) \n",
    "plt.imshow(word_cloud) \n",
    "plt.axis('off') \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b5abd1",
   "metadata": {},
   "source": [
    "## 4. Frequency distribution and polarity\n",
    "### 4a) Create frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c1b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list for tokenised review column.\n",
    "# Define an empty list of tokens review.\n",
    "all_tokens_review = []\n",
    "for i in range(df4.shape[0]):\n",
    "    # Add each token to the list.\n",
    "    all_tokens_review = all_tokens_review + df4['token_review'][i]\n",
    "\n",
    "# Calculate the frequency distribution for review column.\n",
    "freq_dist_of_words = FreqDist(all_tokens_review)\n",
    "\n",
    "# Determine 5 most common words used in reviews.\n",
    "freq_dist_of_words.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b083c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list for tokenised summary column.\n",
    "# Define an empty list of tokens summary.\n",
    "all_tokens_summary = []\n",
    "for i in range(df4.shape[0]):\n",
    "    # Add each token to the list.\n",
    "    all_tokens_summary = all_tokens_summary + df4['token_summary'][i]\n",
    "\n",
    "# Calculate the frequency distribution for summary column.\n",
    "freq_dist_of_words = FreqDist(all_tokens_summary)\n",
    "\n",
    "# Determine 5 most common words used in summary.\n",
    "freq_dist_of_words.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd574d03-a034-454d-b6c5-89aa764c459a",
   "metadata": {},
   "source": [
    "### 4b) Remove alphanumeric characters and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd28216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all the alphanum.\n",
    "token_review = [word for word in all_tokens_review if word.isalnum()]\n",
    "token_summary = [word for word in all_tokens_summary if word.isalnum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a757d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the stopwords for reviews.\n",
    "# Create a set of English stop words.\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Create a filtered list of token_review without stop words.\n",
    "token_review2 = [x for x in token_review if x.lower() not in english_stopwords]\n",
    "\n",
    "# Define an empty string variable for reviews.\n",
    "token_review2_string = ''\n",
    "for value in token_review:\n",
    "    # Add each filtered token word to the string.\n",
    "    token_review2_string = token_review2_string + value + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the stopwords for summary.\n",
    "\n",
    "# Create a filtered list of token_review without stop words.\n",
    "token_summary2 = [x for x in token_summary if x.lower() not in english_stopwords]\n",
    "\n",
    "# Define an empty string variable.\n",
    "token_summary2_string = ''\n",
    "for value in token_summary:\n",
    "    # Add each filtered token word to the string.\n",
    "    token_summary2_string = token_summary2_string + value + ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68b09f-853e-4c9c-8ff9-ba0b33b8c8e3",
   "metadata": {},
   "source": [
    "### 4c) Create wordcloud without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wordcloud object without stopwords for reviews.\n",
    "wordcloud = WordCloud(width = 1600, height = 900, \n",
    "                background_color ='black', \n",
    "                colormap='plasma', \n",
    "                min_font_size = 10).generate(token_review2_string) \n",
    "\n",
    "# Plot the WordCloud image.                        \n",
    "plt.figure(figsize = (16, 9), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis('off') \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wordcloud without stopwords for summary.\n",
    "wordcloud = WordCloud(width = 1600, height = 900, \n",
    "                background_color ='white', \n",
    "                colormap='plasma', \n",
    "                min_font_size = 10).generate(token_summary2_string) \n",
    "\n",
    "# Plot the WordCloud image.                        \n",
    "plt.figure(figsize = (16, 9), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis('off') \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c0b15b",
   "metadata": {},
   "source": [
    "### 4d) Identify 15 most common words and polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the 15 most common words for reviews.\n",
    "# View the frequency distribution for reviews.\n",
    "fdist1 = FreqDist(token_review2)\n",
    "\n",
    "# Preview the data.\n",
    "fdist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83757dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present the data for frequency of words used in reviews in more readable format.\n",
    "\n",
    "# Import the Counter class.\n",
    "from collections import Counter\n",
    "\n",
    "# Generate a DataFrame from Counter for reviews.\n",
    "count_review = pd.DataFrame(Counter(token_review2).most_common(15), \n",
    "                      columns=['Word', 'Frequency']).set_index('Word')\n",
    "\n",
    "# Preview data.\n",
    "count_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae43ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the most frequent words used in review.\n",
    "# Set the plot type.\n",
    "ax = count_review.plot(kind='barh', figsize=(16, 9), fontsize=12,\n",
    "                 colormap ='plasma')\n",
    "\n",
    "# Set the labels.\n",
    "ax.set_xlabel('Count', fontsize=12)\n",
    "ax.set_ylabel('Word', fontsize=12)\n",
    "ax.set_title(\"Reviews: Count of the 15 most frequent words\",\n",
    "             fontsize=20)\n",
    "\n",
    "# Draw the bar labels.\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_width()+.41, i.get_y()+.1, str(round((i.get_width()), 2)),\n",
    "            fontsize=12, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aee796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a DataFrame from Counter for summaries.\n",
    "count_summary = pd.DataFrame(Counter(token_summary2).most_common(15), \n",
    "                      columns=['Word', 'Frequency']).set_index('Word')\n",
    "\n",
    "# Preview data.\n",
    "count_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03473da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the most frequent words used in summary.\n",
    "# Set the plot type.\n",
    "ax = count_summary.plot(kind='barh', figsize=(16, 9), fontsize=12,\n",
    "                 colormap ='plasma')\n",
    "\n",
    "# Set the labels.\n",
    "ax.set_xlabel('Count', fontsize=12)\n",
    "ax.set_ylabel('Word', fontsize=12)\n",
    "ax.set_title(\"Summary: Count of the 15 most frequent words\",\n",
    "             fontsize=20)\n",
    "\n",
    "# Draw the bar labels.\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_width()+.41, i.get_y()+.1, str(round((i.get_width()), 2)),\n",
    "            fontsize=12, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59291784-3505-41e5-9914-e4ec8914524b",
   "metadata": {},
   "source": [
    "## 5. Review polarity and sentiment: Plot histograms of polarity (use 15 bins) and sentiment scores for the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b92f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TextBlob.\n",
    "!pip install textblob\n",
    "\n",
    "# Import the necessary package.\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84288b8f-aab4-4fff-98d0-aaaf5eef28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided function.\n",
    "def generate_polarity(comment):\n",
    "    '''Extract polarity score (-1 to +1) for each comment'''\n",
    "    return TextBlob(comment).sentiment[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ca627",
   "metadata": {},
   "source": [
    "## Review column analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eea3a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine polarity for review column. \n",
    "\n",
    "# Populate a new column with polarity scores for each comment.\n",
    "df4['polarity_review'] = df4['review'].apply(generate_polarity)\n",
    "\n",
    "# Preview the result.\n",
    "df4['polarity_review'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a518be",
   "metadata": {},
   "source": [
    "The function (polarity) works by extracting the relevant score from the sentiment method for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract a subjectivity score for the review.\n",
    "def generate_subjectivity(comment):\n",
    "    return TextBlob(comment).sentiment[1]\n",
    "\n",
    "# Populate a new column with subjectivity scores for each review.\n",
    "df4['subjectivity_review'] = df4['review'].apply(generate_subjectivity)\n",
    "\n",
    "# Preview the result.\n",
    "df4['subjectivity_review'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec3428",
   "metadata": {},
   "source": [
    "The function (subjectivity) works by extracting the relevant score from the sentiment method for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcb45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise sentiment polarity scores for reviews.\n",
    "# Review: Create a histogram plot with bins = 15.\n",
    "# Set the number of bins.\n",
    "num_bins = 15\n",
    "\n",
    "# Set the plot area.\n",
    "plt.figure(figsize=(16,9))\n",
    "\n",
    "# Define the bars.\n",
    "n, bins, patches = plt.hist(df4['polarity_review'], num_bins, facecolor='blue', alpha=0.8)\n",
    "\n",
    "# Set the labels.\n",
    "plt.xlabel('Polarity', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title('Review: Histogram of sentiment score polarity', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d0c511",
   "metadata": {},
   "source": [
    "## Summary column analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9d1928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine polarity for summary column.\n",
    "\n",
    "# Populate a new column with polarity scores for each summary.\n",
    "df4['polarity_summary'] = df4['summary'].apply(generate_polarity)\n",
    "\n",
    "# Preview the result.\n",
    "df4['polarity_summary'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract a subjectivity score for the summary.\n",
    "def generate_subjectivity(comment):\n",
    "    return TextBlob(comment).sentiment[1]\n",
    "\n",
    "# Populate a new column with subjectivity scores for each summary.\n",
    "df4['subjectivity_summary'] = df4['summary'].apply(generate_subjectivity)\n",
    "\n",
    "# Preview the result.\n",
    "df4['subjectivity_summary'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74808b36",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualise sentiment polarity scores for summaries.\n",
    "# Summary: Create a histogram plot with bins = 15.\n",
    "# Set the number of bins.\n",
    "num_bins = 15\n",
    "\n",
    "# Set the plot area.\n",
    "plt.figure(figsize=(16,9))\n",
    "\n",
    "# Define the bars.\n",
    "n, bins, patches = plt.hist(df4['polarity_summary'], num_bins, facecolor='green', alpha=0.8)\n",
    "\n",
    "# Set the labels.\n",
    "plt.xlabel('Polarity', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title('Summary: Histogram of sentiment score polarity', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2523b8",
   "metadata": {},
   "source": [
    "## 6. Identify top 20 positive and negative reviews and summaries respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7c797",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Top 20 negative reviews.\n",
    "# Create a DataFrame for 20 negative reviews.\n",
    "negative_sentiment = df4.nsmallest(20, 'polarity_review')\n",
    "\n",
    "# Eliminate unnecessary columns.\n",
    "negative_sentiment = negative_sentiment[['review', 'polarity_review', 'subjectivity_review']]\n",
    "\n",
    "# View output.\n",
    "negative_sentiment.style.set_properties(subset=['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2388461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sentiment.at[141, 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e172c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 negative summaries.\n",
    "# Create a DataFrame for 20 negative summaries.\n",
    "negative_sentiment = df4.nsmallest(20, 'polarity_summary')\n",
    "\n",
    "# Eliminate unnecessary columns.\n",
    "negative_sentiment = negative_sentiment[['summary', 'polarity_summary', 'subjectivity_summary']]\n",
    "\n",
    "# View output.\n",
    "negative_sentiment.style.set_properties(subset=['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348d8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 positive reviews.\n",
    "# Create a DataFrame for 20 most positive reviews.\n",
    "positive_sentiment = df4.nlargest(20, 'polarity_review')\n",
    "\n",
    "# Eliminate unnecessary columns.\n",
    "positive_sentiment = positive_sentiment[['review', 'polarity_review', 'subjectivity_review']]\n",
    "\n",
    "# View the output.\n",
    "positive_sentiment.style.set_properties(subset=['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 positive summaries.\n",
    "# Create a DataFrame for 20 most positive summaries.\n",
    "positive_sentiment = df4.nlargest(20, 'polarity_summary')\n",
    "\n",
    "# Eliminate unnecessary columns.\n",
    "positive_sentiment = positive_sentiment[['summary', 'polarity_summary', 'subjectivity_summary']]\n",
    "\n",
    "# View output.\n",
    "positive_sentiment.style.set_properties(subset=['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b2a108-a8af-4164-9b02-40068c17836d",
   "metadata": {},
   "source": [
    "## 7. Discuss: Insights and observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281f148",
   "metadata": {},
   "source": [
    "Top 15 words used in reviews and summaries are mostly positive ones. Lots of mentioning game, fun, play, love etc.\n",
    "\n",
    "Reviews: This histogram shows us that most reviews located in positive zone, so most of the users are expressing positive sentiment in their reviews.\n",
    "\n",
    "Summary: Histogram shows us that most summaries sit in a positive part of the plot: users show a particularly strong sentiment in positive direction.\n",
    "\n",
    "Top 20 positive and negative reviews and summaries. After checking how NLP did sentimental analysis, we can conclude, that human recheck is needed especially for the negative ones. Complaints about missing parts and product quality should be addressed. Sometimes NLP misinterprets a game description (especially when it containts agressive or violent content) as a product review as well as it doesn't pick up sarcasm or jokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0229bcbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a182db62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
